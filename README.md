# Zero-Shot Robustness Analysis & Prompt Engineering with CLIP

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![OpenCLIP](https://img.shields.io/badge/OpenCLIP-ViT--B--32-green)
![Status](https://img.shields.io/badge/Status-Completed-success)

## üìå Project Overview
This project investigates the **Zero-Shot classification capabilities** of Vision-Language Models (VLMs), specifically **OpenCLIP (ViT-B-32)**, on the **CIFAR-10** dataset. 

Unlike traditional supervised learning, this project utilizes **Prompt Engineering** to align textual and visual feature spaces without any model fine-tuning. The study analyzes the impact of different prompt templates on accuracy and performs a deep-dive error analysis to understand semantic misclassifications.

## üöÄ Key Features
* **Zero-Shot Inference:** Implementation of an OpenCLIP pipeline to classify images using natural language prompts.
* **Prompt Engineering Experiments:** Systematic evaluation of 5 distinct prompt templates to optimize semantic alignment.
* **Ensemble Prompting:** Implementation of class-specific embedding averaging to boost robustness.
* **Advanced Error Analysis:** Generation of Confusion Matrices and Radar Charts to visualize per-class weaknesses (e.g., biological vs. mechanical objects).

## üìä Methodology & Data
* **Model:** `ViT-B-32` pretrained on `LAION-2B` (via OpenCLIP).
* **Dataset:** CIFAR-10 Test Set (10,000 images, balanced 10 classes).
* **Technique:** Cosine similarity calculation between normalized image embeddings and text prompt embeddings.

## üß™ Experiments & Results

### 1. Prompt Engineering Comparison
We tested varying prompt structures to determine which linguistic cues best retrieved correct visual features.

| Prompt Template | Accuracy | Insight |
| :--- | :--- | :--- |
| **"a drawing of a {}"** | **87.54%** | **Best Performance** |
| "a photo of a large {}" | 87.40% | Strong contender |
| "a photo of a {}" | 86.17% | Baseline |
| "an image of a {}" | 86.06% | Neutral |
| "a photo of a small {}" | 84.65% | Lowest performance |

*Finding: The model responds better to "drawing" than "photo" on this dataset, suggesting the pre-training data (LAION-2B) may contain strong artistic/illustrative representations of these classes.*

### 2. Class-Specific Performance (Best Prompt)
Using the best prompt (`"a drawing of a {}"`), we analyzed error rates per class.

* **Top Performers:** Truck (1.9% error), Ship (3.1% error), Horse (3.8% error).
* **Challenging Classes:** Frog (27.5% error), Cat (20.5% error).

### 3. Misclassification Patterns
An analysis of the confusion matrix revealed semantic clustering:
* **Cats** are most frequently confused with **Dogs** (129 instances).
* **Automobiles** are most frequently confused with **Trucks** (120 instances).
* **Frogs** are confused with **Birds** and **Deer** (likely due to background textures).

## üñºÔ∏è Visualizations
*(Note: These plots are generated by the notebook)*

* **Class Distribution:** Verified perfect balance (1000 images/class).
* **Confusion Matrix:** Heatmap showing where the model gets confused.
* **Radar Chart:** Polar plot visualizing the "Error Rate" spike for Frogs and Cats.

## üõ†Ô∏è Installation & Usage

1. **Clone the repository**
   ```bash
   git clone [https://github.com/yourusername/clip-zero-shot-analysis.git](https://github.com/yourusername/clip-zero-shot-analysis.git)
   cd clip-zero-shot-analysis
